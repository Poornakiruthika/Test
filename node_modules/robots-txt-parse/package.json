{
  "_args": [
    [
      {
        "raw": "robots-txt-parse@~0.0.4",
        "scope": null,
        "escapedName": "robots-txt-parse",
        "name": "robots-txt-parse",
        "rawSpec": "~0.0.4",
        "spec": ">=0.0.4 <0.1.0",
        "type": "range"
      },
      "D:\\dev\\nightwatch\\node_modules\\nightwatch\\node_modules\\broken-link-checker"
    ]
  ],
  "_from": "robots-txt-parse@>=0.0.4 <0.1.0",
  "_id": "robots-txt-parse@0.0.4",
  "_inCache": true,
  "_location": "/robots-txt-parse",
  "_nodeVersion": "5.0.0",
  "_npmUser": {
    "name": "janpotoms",
    "email": "potoms.jan@gmail.com"
  },
  "_npmVersion": "3.3.6",
  "_phantomChildren": {},
  "_requested": {
    "raw": "robots-txt-parse@~0.0.4",
    "scope": null,
    "escapedName": "robots-txt-parse",
    "name": "robots-txt-parse",
    "rawSpec": "~0.0.4",
    "spec": ">=0.0.4 <0.1.0",
    "type": "range"
  },
  "_requiredBy": [
    "/broken-link-checker"
  ],
  "_resolved": "https://registry.npmjs.org/robots-txt-parse/-/robots-txt-parse-0.0.4.tgz",
  "_shasum": "f7d1f323f79921d7e9c6c4bbd25048f6e9810d71",
  "_shrinkwrap": null,
  "_spec": "robots-txt-parse@~0.0.4",
  "_where": "D:\\dev\\nightwatch\\node_modules\\nightwatch\\node_modules\\broken-link-checker",
  "author": {
    "name": "Jan Potoms"
  },
  "dependencies": {
    "bluebird": "^2.3.5",
    "split": "^0.3.0",
    "stream-combiner": "^0.2.1",
    "through": "^2.3.4"
  },
  "description": "Streaming parser for robots.txt files",
  "devDependencies": {
    "chai": "^1.9.1",
    "mocha": "^1.18.2"
  },
  "directories": {},
  "dist": {
    "shasum": "f7d1f323f79921d7e9c6c4bbd25048f6e9810d71",
    "tarball": "https://registry.npmjs.org/robots-txt-parse/-/robots-txt-parse-0.0.4.tgz"
  },
  "gitHead": "c8b0683233928973d4abfb0d7085beaa6984e661",
  "license": "MIT",
  "main": "lib/parse.js",
  "maintainers": [
    {
      "name": "janpotoms",
      "email": "potoms.jan@gmail.com"
    }
  ],
  "name": "robots-txt-parse",
  "optionalDependencies": {},
  "readme": "# robots-txt-parse [![Build Status](https://travis-ci.org/Woorank/robots-txt-parse.svg)](https://travis-ci.org/Woorank/robots-txt-parse)\n\nStreaming robots.txt parser\n\n## usage\n\n```js\nvar parse = require('robots-txt-parse'),\n    fs    = require('fs');\n\nparse(fs.createReadStream(__dirname + '/robots.txt'))\n  .then(function (robots) {\n    console.log(robots)\n  });\n\n```\nassuming this file\n```\nuser-agent: *\nuser-agent: googlebot\ndisallow: /\n\nuser-agent: twitterbot\ndisallow: /\nallow: /twitter\n\nSitemap: http://www.example.com/sitemap.xml\n```\nproduces following output\n```json\n{\n  \"groups\": [{\n    \"agents\": [ \"*\", \"googlebot\" ],\n    \"rules\": [\n      { \"rule\": \"disallow\", \"path\": \"/\" }\n    ]\n  }, {\n    \"agents\": [ \"twitterbot\" ],\n    \"rules\": [\n      { \"rule\": \"disallow\", \"path\": \"/\" },\n      { \"rule\": \"allow\", \"path\": \"/twitter\" }\n    ]\n  }],\n  \"extensions\": [\n    { \"extension\": \"sitemap\", \"value\": \"http://www.example.com/sitemap.xml\" }\n  ]\n}\n```",
  "readmeFilename": "README.md",
  "scripts": {
    "test": "mocha -R spec ./test",
    "test-watch": "mocha -w -R spec ./test"
  },
  "version": "0.0.4"
}
